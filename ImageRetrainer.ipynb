{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN for Art Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network is trained to classify the art of the following 10 artists: **Bernard Picart**, **George Hendrik Breitner**, **Isaac Israels**, **Jan Luyken**, **Johannes Tavenraat**, **Marius Bauer**, **Reinier Vinkeles**, **Rembrandt Harmensz van Rijn**, **Simon Fokke**, **Willem Witsen**. It extends the research done by Stanford at http://cs231n.stanford.edu/reports/2017/pdfs/410.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network is based on the imagenet \"inception-model\" and is retrained on 1000 images of artwork, 100 from each artist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./dataset/RembrandtHarmenszvanRijn/en-SK-A-1935.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./dataset/RembrandtHarmenszvanRijn/en-SK-C-5.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./dataset/BernardPicart/en-BK-NM-12896.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "import tarfile\n",
    "import re\n",
    "import random\n",
    "import hashlib\n",
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.util import compat\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inception_url = 'http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz'\n",
    "inception_dir_filepath = './inception/'\n",
    "inception_filepath = inception_dir_filepath + 'classify_image_graph_def.pb'\n",
    "train_graph_dir_filepath = './train'\n",
    "input_tensor_name = 'Mul:0'\n",
    "second_last_tensor_name = 'pool_3/_reshape:0'\n",
    "second_last_tensor_size = 2048\n",
    "MAX_NUM_IMAGES_PER_CLASS = 2 ** 27 - 1\n",
    "\n",
    "num_training_steps = 2000\n",
    "train_batch_size = 100\n",
    "eval_step_interval = 10\n",
    "testing_percentage = 20\n",
    "validation_percentage = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`NeuralNetwork` is the main class in the project so many of the key functions are in this class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    def __init__(self, images_dict):\n",
    "        self.images_dict = images_dict\n",
    "        graph = tf.Graph()\n",
    "\n",
    "        with graph.as_default():\n",
    "            with tf.gfile.FastGFile(inception_filepath, 'rb') as f:\n",
    "                graph_def = tf.GraphDef()\n",
    "                graph_def.ParseFromString(f.read())\n",
    "                self.input_tensor, bottleneck_input = tf.import_graph_def(graph_def, name='', return_elements=[input_tensor_name, second_last_tensor_name])\n",
    "\n",
    "        self.session = tf.Session(graph=graph)\n",
    "\n",
    "        with self.session as sess:\n",
    "            self.jpeg_data_tensor, self.decoded_image_tensor = self.add_jpeg_decoding(299, 299, 3, 128, 128)\n",
    "            self.add_final_layer(bottleneck_input, len(self.images_dict.keys()))\n",
    "            self.add_evaluation_step()\n",
    "\n",
    "            self.merged = tf.summary.merge_all()\n",
    "            if not os.path.exists(train_graph_dir_filepath):\n",
    "                os.makedirs(train_graph_dir_filepath)\n",
    "            self.train_writer = tf.summary.FileWriter(train_graph_dir_filepath, graph)\n",
    "\n",
    "            init = tf.global_variables_initializer()\n",
    "            sess.run(init)\n",
    "\n",
    "            self.layer = graph.get_tensor_by_name('conv:0')\n",
    "            for i in graph.get_operations():\n",
    "                print(i.values())\n",
    "\n",
    "            self.train_network(num_training_steps)\n",
    "            \n",
    "    # the art images are in jpeg format which our network cannot use directly, so we decode the image into a useable format     \n",
    "    def add_jpeg_decoding(self, input_width, input_height, input_depth, input_mean, input_std):\n",
    "        jpeg_data = tf.placeholder(tf.string, name='decode_jpg_input')\n",
    "        decoded_image = tf.image.decode_jpeg(jpeg_data, channels=input_depth)\n",
    "        decoded_image_as_float = tf.cast(decoded_image, dtype=tf.float32)\n",
    "        decoded_image_4d = tf.expand_dims(decoded_image_as_float, 0)\n",
    "        resize_shape = tf.stack([input_height, input_width])\n",
    "        resize_shape_as_int = tf.cast(resize_shape, dtype=tf.int32)\n",
    "        resized_image = tf.image.resize_bilinear(decoded_image_4d,\n",
    "                                               resize_shape_as_int)\n",
    "        offset_image = tf.subtract(resized_image, input_mean)\n",
    "        mul_image = tf.multiply(offset_image, 1.0 / input_std)\n",
    "\n",
    "        return jpeg_data, mul_image\n",
    "    \n",
    "    # the pre-trained inception model does not have a final layer, so we must add one suitable for artist classification\n",
    "    def add_final_layer(self, bottleneck_input, class_count):\n",
    "        self.bottleneck_tensor = tf.placeholder_with_default(bottleneck_input, shape=[None, second_last_tensor_size], name='input_placeholder')\n",
    "        self.ground_truth_tensor = tf.placeholder(tf.float32, [None, class_count], name='ground_truth_input')\n",
    "\n",
    "        layer_weights = tf.Variable(tf.truncated_normal([second_last_tensor_size, class_count], stddev=0.001), name='final_weights')\n",
    "        layer_biases = tf.Variable(tf.zeros([class_count]), name='final_biases')\n",
    "\n",
    "        logits = tf.add(tf.matmul(self.bottleneck_tensor, layer_weights), layer_biases)\n",
    "\n",
    "        self.final_tensor = tf.nn.softmax(logits, name='final_result')\n",
    "\n",
    "        with tf.name_scope('cross_entropy'):\n",
    "            cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=self.ground_truth_tensor, logits=logits)\n",
    "            with tf.name_scope('total'):\n",
    "                cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
    "        tf.summary.scalar('cross_entropy', cross_entropy_mean)\n",
    "\n",
    "        with tf.name_scope('train'):\n",
    "            optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "            self.train_step = optimizer.minimize(cross_entropy_mean)\n",
    "            \n",
    "    # used to evaluate the accuracy of the model\n",
    "    def add_evaluation_step(self):\n",
    "        with tf.name_scope('accuracy'):\n",
    "            with tf.name_scope('correct_prediction'):\n",
    "                self.prediction = tf.argmax(self.final_tensor, 1)\n",
    "                correct_prediction = tf.equal(self.prediction, tf.argmax(self.ground_truth_tensor, 1))\n",
    "            self.evaluation_step = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "        tf.summary.scalar('accuracy', self.evaluation_step)\n",
    "        \n",
    "    # used to train the model by randomly fetching a train_batch_size amount of images for num_training_steps\n",
    "    def train_network(self, num_training_steps):\n",
    "        print(\"Starting...\")\n",
    "\n",
    "        for i in range(num_training_steps):\n",
    "            train_bottlenecks, train_ground_truths = self.get_tensors_for_training('training')\n",
    "\n",
    "            train_summary, _ = self.session.run([self.merged, self.train_step], feed_dict={self.bottleneck_tensor: train_bottlenecks, self.ground_truth_tensor: train_ground_truths})\n",
    "            self.train_writer.add_summary(train_summary, i)\n",
    "\n",
    "            if (i % eval_step_interval) == 0:\n",
    "                train_accuracy = self.session.run([self.evaluation_step], feed_dict={self.bottleneck_tensor: train_bottlenecks, self.ground_truth_tensor: train_ground_truths})[0]\n",
    "                print('Step %d: Train accuracy = %.1f%%' % (i, train_accuracy * 100))\n",
    "\n",
    "        test_bottlenecks, test_ground_truths = self.get_tensors_for_testing('testing')\n",
    "        test_accuracy, predictions = self.session.run([self.evaluation_step, self.prediction], feed_dict={\n",
    "            self.bottleneck_tensor: test_bottlenecks, self.ground_truth_tensor: test_ground_truths})\n",
    "        print('Final test accuracy = %.1f%% (N=%d)' % (test_accuracy * 100, len(test_bottlenecks)))\n",
    "\n",
    "        self.getActivations()\n",
    "        \n",
    "    def get_tensors_for_training(self, category):\n",
    "        class_count = len(self.images_dict.keys())\n",
    "        tensor_inputs, ground_truth_inputs = [], []\n",
    "\n",
    "        for unused_i in range(train_batch_size):\n",
    "            label_index = random.randrange(class_count)\n",
    "            label_name = list(self.images_dict.keys())[label_index]\n",
    "\n",
    "            image_index = random.randrange(MAX_NUM_IMAGES_PER_CLASS + 1)\n",
    "            image_path = self.get_image_path(label_name, image_index, category)\n",
    "            image_data = tf.gfile.FastGFile(image_path, 'rb').read()\n",
    "            tensor_input = self.run_bottleneck_on_image(image_data)\n",
    "\n",
    "            ground_truth_input = np.zeros(class_count, dtype=np.float32)\n",
    "            ground_truth_input[label_index] = 1.0\n",
    "\n",
    "            tensor_inputs.append(tensor_input)\n",
    "            ground_truth_inputs.append(ground_truth_input)\n",
    "\n",
    "        return tensor_inputs, ground_truth_inputs\n",
    "    \n",
    "    def get_tensors_for_testing(self, category):\n",
    "        class_count = len(self.images_dict.keys())\n",
    "        bottlenecks, ground_truths = [], []\n",
    "        for label_index, label_name in enumerate(self.images_dict.keys()):\n",
    "            for image_index, image_name in enumerate(self.images_dict[label_name][category]):\n",
    "                image_path = self.get_image_path(label_name, image_index, category)\n",
    "                image_data = tf.gfile.FastGFile(image_path, 'rb').read()\n",
    "                bottleneck = self.run_bottleneck_on_image(image_data)\n",
    "\n",
    "                ground_truth = np.zeros(class_count, dtype=np.float32)\n",
    "                ground_truth[label_index] = 1.0\n",
    "\n",
    "                bottlenecks.append(bottleneck)\n",
    "                ground_truths.append(ground_truth)\n",
    "\n",
    "        return bottlenecks, ground_truths\n",
    "    \n",
    "    def get_image_path(self, label_name, index, category):\n",
    "        label_lists = self.images_dict[label_name]\n",
    "        category_list = label_lists[category]\n",
    "        mod_index = index % len(category_list)\n",
    "        base_name = category_list[mod_index]\n",
    "        full_path = os.path.join('./dataset', label_name, base_name)\n",
    "        return full_path\n",
    "    \n",
    "    def run_bottleneck_on_image(self, image_data):\n",
    "        # First decode the JPEG image, resize it, and rescale the pixel values.\n",
    "        resized_input_values = self.session.run(self.decoded_image_tensor, {self.jpeg_data_tensor: image_data})\n",
    "        # Then run it through the recognition network.\n",
    "        bottleneck_values = self.session.run(self.bottleneck_tensor, {self.input_tensor: resized_input_values})\n",
    "        bottleneck_values = np.squeeze(bottleneck_values)\n",
    "        return bottleneck_values\n",
    "    \n",
    "    def getActivations(self):\n",
    "        image_path = self.get_image_path(list(self.images_dict.keys())[0], 0, 'validation')\n",
    "        image_data = tf.gfile.FastGFile(image_path, 'rb').read()\n",
    "        resized_input_values = self.session.run(self.decoded_image_tensor, {self.jpeg_data_tensor: image_data})\n",
    "\n",
    "        units = self.session.run(self.layer, {self.input_tensor: resized_input_values})\n",
    "        self.plotNNFilter(units)\n",
    "        \n",
    "    def plotNNFilter(self, units):\n",
    "        filters = units.shape[3]\n",
    "        plt.figure(1, figsize=(20,20))\n",
    "        n_columns = 6\n",
    "        n_rows = math.ceil(filters / n_columns) + 1\n",
    "\n",
    "        for i in range(filters):\n",
    "            plt.subplot(n_rows, n_columns, i+1)\n",
    "            plt.title('Filter ' + str(i))\n",
    "            plt.imshow(units[0,:,:,i], interpolation=\"nearest\", cmap=\"gray\")\n",
    "        plt.savefig('filters.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create arrays of training and testing images\n",
    "def initialize_images():\n",
    "    images_dict = {}\n",
    "    artist_dirs = [x[0] for i, x in enumerate(os.walk('./dataset')) if i != 0]\n",
    "\n",
    "    for artist_dir in artist_dirs:\n",
    "        artist_name = os.path.basename(artist_dir)\n",
    "        artist_images = filter(lambda f: f.split('.')[-1] in ['jpg', 'jpeg', 'JPG', 'JPEG'], os.listdir(artist_dir))\n",
    "\n",
    "        validation_images, testing_images, training_images = [], [], []\n",
    "\n",
    "        for artist_image in artist_images:\n",
    "            base_name = os.path.basename(artist_image)\n",
    "            hash_name = re.sub(r'_nohash_.*$', '', base_name)\n",
    "            hash_name_hashed = hashlib.sha1(compat.as_bytes(hash_name)).hexdigest()\n",
    "            percentage_hash = ((int(hash_name_hashed, 16) % (MAX_NUM_IMAGES_PER_CLASS + 1)) * (100.0 / MAX_NUM_IMAGES_PER_CLASS))\n",
    "\n",
    "            if percentage_hash < validation_percentage:\n",
    "                validation_images.append(base_name)\n",
    "            elif percentage_hash < (testing_percentage + validation_percentage):\n",
    "                testing_images.append(base_name)\n",
    "            else:\n",
    "                training_images.append(base_name)\n",
    "\n",
    "        images_dict[artist_name] = {\n",
    "            'training': training_images,\n",
    "            'testing': testing_images,\n",
    "            'validation': validation_images\n",
    "        }\n",
    "\n",
    "    return images_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We the run script starting with `initialize_images` and `initialize_model`. For the purposes of this notebook however we have commented out the call to start the program. \n",
    "\n",
    "Running it would re-train the entire network which takes several hours.\n",
    "\n",
    "Instead the output is replicated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # model = initialize_model(initialize_images())\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Starting...\n",
    "Step 0: Train accuracy = 16.0%\n",
    "Step 10: Train accuracy = 42.0%\n",
    "Step 20: Train accuracy = 65.0%\n",
    "Step 30: Train accuracy = 61.0%\n",
    "Step 40: Train accuracy = 69.0%\n",
    "Step 50: Train accuracy = 65.0%\n",
    "Step 60: Train accuracy = 73.0%\n",
    "Step 70: Train accuracy = 69.0%\n",
    "Step 80: Train accuracy = 82.0%\n",
    "Step 90: Train accuracy = 70.0%\n",
    "Step 100: Train accuracy = 66.0%\n",
    "...\n",
    "Step 1900: Train accuracy = 98.0%\n",
    "Step 1910: Train accuracy = 96.0%\n",
    "Step 1920: Train accuracy = 97.0%\n",
    "Step 1930: Train accuracy = 98.0%\n",
    "Step 1940: Train accuracy = 99.0%\n",
    "Step 1950: Train accuracy = 96.0%\n",
    "Step 1960: Train accuracy = 98.0%\n",
    "Step 1970: Train accuracy = 96.0%\n",
    "Step 1980: Train accuracy = 97.0%\n",
    "Step 1990: Train accuracy = 97.0%\n",
    "Final test accuracy = 70.7% (N=198)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy and Cross Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the graph below that accuracy against the training data rises sharply during training, then gradually approaches 100%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![accuracy](./visualizations/accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross entropy trends approximately inversely to accuracy as we would expect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![cross entropy](./visualizations/cross_entropy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weights and Biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plots below represent how the model's weights and biases changed during training. \n",
    "\n",
    "Each plot is made of many histograms. The ones further in the \"background\" of the image are the older histograms, the foreground histograms are more recent in training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the weights start out very uniform as we expect, and then the distribution widens during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![weights](./visualizations/weights.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The change to the biases during training is more pronounced. The distribution becomes divided into approximately three modes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![biases](./visualizations/biases.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the activation maps for various filters for a given input image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![original](./visualizations/activations_5/en-RP-P-1909-1642.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![all filters](./visualizations/activations_5/filters.png)\n",
    "![filter0](./visualizations/activations_5/filter_6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![original](./visualizations/activations_0/en-RP-P-1927-282.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![all filters](./visualizations/activations_0/filters.png)\n",
    "![filter0](./visualizations/activations_0/filter_11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![original](./visualizations/activations_9/en-RP-T-1930-17.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![all filters](./visualizations/activations_9/filters.png)\n",
    "![filter0](./visualizations/activations_9/filter_1.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
